{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi With Rules: [('Advertising', 'VERB'), ('is', 'VERB'), ('a', 'DET'), ('good', 'ADJ'), ('way', 'NOUN'), ('for', 'ADP'), ('advertisement', 'NOUN')]\n",
      "Viterbi Without Rules: [('Advertising', 'PRON'), ('is', 'VERB'), ('a', 'DET'), ('good', 'ADJ'), ('way', 'NOUN'), ('for', 'ADP'), ('advertisement', 'PRON')]\n",
      "\n",
      "Viterbi With Rule Accuracy: 90.71%\n",
      "Viterbi Without Rule Accuracy: 71.58%\n",
      "\n",
      "Viterbi With Rule Runtime: 1.101s\n",
      "Viterbi Without Rule Runtime: 1.041s\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "# from nltk.corpus import brown     # assuming that you've download brown corpus\n",
    " \n",
    "#download the brown corpus from nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "#download the universal tagset from nltk\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.brown.tagged_sents(tagset='universal'))[:300]\n",
    "\n",
    "# split data into training and validation set in the ratio 80:20\n",
    "train_set,test_set =train_test_split(nltk_data,train_size=0.80,test_size=0.20,random_state = 101)\n",
    "\n",
    "# create list of train and test tagged words\n",
    "train_tagged_words = [ tup for sent in train_set for tup in sent ]\n",
    "test_tagged_words = [ tup for sent in test_set for tup in sent ]\n",
    "\n",
    "# check some of the tagged words.\n",
    "train_tagged_words[:5]\n",
    "\n",
    "#use set datatype to check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in train_tagged_words}\n",
    " \n",
    "# check total words in vocabulary\n",
    "vocab = {word for word,tag in train_tagged_words}\n",
    "\n",
    "# compute Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)#total number of times the passed tag occurred in train_bag\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "#now calculate the total number of times the passed word occurred as the passed tag.\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    " \n",
    "     \n",
    "    return (count_w_given_tag, count_tag)\n",
    "\n",
    "# compute  Transition Probability\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)\n",
    "\n",
    "# creating t x t transition matrix of tags, t= no of tags\n",
    "# Matrix(i, j) represents P(jth tag after the ith tag)\n",
    " \n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
    " \n",
    "# convert the matrix to a df for better readability\n",
    "#the table is same as the transition table shown in section 3 of article\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
    "\n",
    "#### End emission and transition probability#### \n",
    "\n",
    "#### Start viterbi without rule ####\n",
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "     \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                 \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "             \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))\n",
    "\n",
    "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
    "random.seed(1234)      #define a random seed to get same sentences when run multiple times\n",
    " \n",
    "# choose random 10 numbers\n",
    "rndom = [random.randint(1,len(test_set)) for x in range(10)]\n",
    " \n",
    "# list of 10 sents on which we test the model\n",
    "test_run = [test_set[i] for i in rndom]\n",
    " \n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    " \n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "\n",
    "#Here We will only test 10 sentences to check the accuracy\n",
    "#as testing the whole training set takes huge amount of time\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    " \n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    " \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "\n",
    "# CHECK FOR ACCURACY FIRST\n",
    "#### End viterbi without rule ####\n",
    "\n",
    "\n",
    "#### Start viterbi with rule ####\n",
    "\n",
    "#To improve the performance,we specify a rule base tagger for unknown words \n",
    "# specify patterns for tagging\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VERB'),              # gerund\n",
    "    (r'.*ed$', 'VERB'),               # past tense \n",
    "    (r'.*es$', 'VERB'),               # verb    \n",
    "    (r'.*\\'s$', 'NOUN'),              # possessive nouns\n",
    "    (r'.*s$', 'NOUN'),                # plural nouns\n",
    "    (r'\\*T?\\*?-[0-9]+$', 'X'),        # X\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'), # cardinal numbers\n",
    "    (r'.*', 'NOUN')                   # nouns\n",
    "]\n",
    " \n",
    "# rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "\n",
    "#modified Viterbi to include rule based tagger in it\n",
    "def Viterbi_rule_based(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "     \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                 \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "             \n",
    "        pmax = max(p)\n",
    "        state_max = rule_based_tagger.tag([word])[0][1]       \n",
    "        \n",
    "         \n",
    "        if(pmax==0):\n",
    "            state_max = rule_based_tagger.tag([word])[0][1] # assign based on rule based tagger\n",
    "        else:\n",
    "            if state_max != 'X':\n",
    "                # getting state for which probability is maximum\n",
    "                state_max = T[p.index(pmax)]                \n",
    "             \n",
    "         \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))\n",
    "\n",
    "#test accuracy on subset of test data \n",
    "start = time.time()\n",
    "process = Viterbi_rule_based(test_tagged_words)\n",
    "end = time.time()\n",
    "time_difference_with_rule = end-start\n",
    "\n",
    "# accuracy with rule\n",
    "tagged_seq_rule = Viterbi_rule_based(test_tagged_words)\n",
    "check_with_rule = [i for i, j in zip(tagged_seq_rule, test_run_base) if i == j]\n",
    "accuracy_with_rule = len(check_with_rule)/len(tagged_seq_rule)\n",
    "\n",
    "# test time\n",
    "start = time.time()\n",
    "process = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "time_difference_without_rule = end-start\n",
    "\n",
    "# accuracy without rule\n",
    "tagged_seq_without_rule = Viterbi(test_tagged_words)\n",
    "check_without_rule = [i for i, j in zip(tagged_seq_without_rule, test_run_base) if i == j]\n",
    "accuracy_without_rule = len(check_without_rule) / len(tagged_seq_without_rule)\n",
    " \n",
    "#### CHECK FOR ACCURACY \n",
    "\n",
    "#Check how a sentence is tagged by the two POS taggers\n",
    "#and compare them\n",
    "test_sent=\"Advertising is a good way for advertisement\"\n",
    "pred_tags_rule=Viterbi_rule_based(test_sent.split())\n",
    "pred_tags_withoutRules= Viterbi(test_sent.split())\n",
    "# print(T)\n",
    "\n",
    "# Tests\n",
    "print(\"Viterbi With Rules:\", pred_tags_rule)\n",
    "print(\"Viterbi Without Rules:\", pred_tags_withoutRules)\n",
    "print()\n",
    "print(f\"Viterbi With Rule Accuracy: {str(accuracy_with_rule * 100)[:5]}%\")\n",
    "print(f\"Viterbi Without Rule Accuracy: {str(accuracy_without_rule * 100)[:5]}%\")\n",
    "print()\n",
    "print(f\"Viterbi With Rule Runtime: {str(time_difference_with_rule)[:5]}s\")\n",
    "print(f\"Viterbi Without Rule Runtime: {str(time_difference_without_rule)[:5]}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
